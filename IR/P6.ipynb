{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Page Rank Algorithm. (Use python or beautiful soup for implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank values:\n",
      "https://example.com/page1: 0.0500\n",
      "https://example.com/page2: 0.0500\n",
      "https://example.com/page3: 0.0500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Fetch links from each URL\n",
    "def fetch_links(url):\n",
    "    \"\"\"\n",
    "    Takes a URL as input, fetches its HTML content, and extracts all absolute hyperlinks (starting with 'http').\n",
    "    Returns:\n",
    "        A set of unique links found on the given URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = set()  # Using a set to avoid duplicate links\n",
    "\n",
    "        # The soup.find_all('a', href=True) command finds all <a> tags in the HTML \n",
    "        # that have an href attribute (which is where links are typically stored).\n",
    "\n",
    "        # For each <a> tag found, the href attribute (the link) is extracted using link.get('href') \n",
    "        # and stored in the href variable.\n",
    "\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link.get('href')\n",
    "            if href and href.startswith('http'):  # Filtering absolute links only\n",
    "                links.add(href)\n",
    "        \n",
    "        # The code checks if the href is not empty (if href) and \n",
    "        # whether it starts with \"http\" (href.startswith('http')). \n",
    "        # This ensures that only absolute URLs (those beginning with \"http\") \n",
    "        # are added to the links set, ignoring relative links.\n",
    "\n",
    "        # If the link meets the criteria, it is added to the links set using links.add(href).\n",
    "\n",
    "        \n",
    "        return links\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching links from {url}: {e}\")\n",
    "        return set()\n",
    "\n",
    "# Step 2: Build the web graph as a dictionary of URLs with their outgoing links\n",
    "def build_web_graph(urls):\n",
    "    \"\"\"\n",
    "    Constructs a dictionary where each URL points to a set of outgoing links from that URL.\n",
    "    Args:\n",
    "        urls: A list of URLs to analyze.\n",
    "    Returns:\n",
    "        A dictionary representing the web graph.\n",
    "    \"\"\"\n",
    "    web_graph = {}\n",
    "    for url in urls:\n",
    "        links = fetch_links(url)\n",
    "        web_graph[url] = links\n",
    "    return web_graph\n",
    "\n",
    "# Step 3: Create the adjacency matrix from the web graph\n",
    "def create_adjacency_matrix(web_graph):\n",
    "    \"\"\"\n",
    "    Converts the web graph dictionary into an adjacency matrix, where each element at (i, j) represents\n",
    "    the link probability from URL i to URL j.\n",
    "    Returns:\n",
    "        adjacency_matrix: The transition matrix used for PageRank.\n",
    "        nodes: A list of URLs representing each index in the adjacency matrix.\n",
    "    \"\"\"\n",
    "    nodes = list(web_graph.keys())  # List of all nodes (URLs)\n",
    "    n = len(nodes)\n",
    "    node_to_index = {node: i for i, node in enumerate(nodes)}\n",
    "    adjacency_matrix = np.zeros((n, n))  # Initializing a square matrix of size n x n\n",
    "\n",
    "    for i, url in enumerate(nodes):\n",
    "        links = web_graph[url]\n",
    "        if links:\n",
    "            for link in links:\n",
    "                if link in node_to_index:\n",
    "                    j = node_to_index[link]  # Mapping URL to index\n",
    "                    adjacency_matrix[i, j] = 1 / len(links)\n",
    "        else:\n",
    "            # Handling dangling nodes (pages with no outbound links)\n",
    "            adjacency_matrix[i, :] = 1 / n  # Equal probability for each page\n",
    "    \n",
    "    return adjacency_matrix, nodes\n",
    "\n",
    "# Step 4: Calculate PageRank using the adjacency matrix\n",
    "def calculate_pagerank(adjacency_matrix, alpha=0.85, max_iter=100, tol=1.0e-6):\n",
    "    \"\"\"\n",
    "    Uses the power iteration method to compute the PageRank vector.\n",
    "    Args:\n",
    "        adjacency_matrix: The transition matrix for web pages.\n",
    "        alpha: The damping factor (probability of random jump).\n",
    "        max_iter: The maximum number of iterations.\n",
    "        tol: Convergence tolerance.\n",
    "    Returns:\n",
    "        A numpy array containing the PageRank scores for each URL.\n",
    "    \"\"\"\n",
    "    n = adjacency_matrix.shape[0]\n",
    "    pagerank = np.ones(n) / n  # Start with an equal probability distribution\n",
    "    teleport = np.ones(n) / n  # Teleportation vector to handle dead-ends and random jumps\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        new_pagerank = alpha * adjacency_matrix @ pagerank + (1 - alpha) * teleport\n",
    "        # Check if the algorithm has converged\n",
    "        if np.linalg.norm(new_pagerank - pagerank, 1) < tol:\n",
    "            break\n",
    "        pagerank = new_pagerank\n",
    "    \n",
    "    return pagerank\n",
    "\n",
    "# List of URLs to analyze\n",
    "urls = [\n",
    "    \"https://example.com/page1\",\n",
    "    \"https://example.com/page2\",\n",
    "    \"https://example.com/page3\"\n",
    "]\n",
    "\n",
    "# Step 1: Build the web graph from the list of URLs\n",
    "web_graph = build_web_graph(urls)\n",
    "\n",
    "# Step 2: Create the adjacency matrix from the web graph\n",
    "adjacency_matrix, nodes = create_adjacency_matrix(web_graph)\n",
    "\n",
    "# Step 3: Calculate PageRank\n",
    "pagerank_values = calculate_pagerank(adjacency_matrix)\n",
    "\n",
    "# Display the PageRank values for each URL\n",
    "print(\"PageRank values:\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f\"{node}: {pagerank_values[i]:.4f}\")\n",
    "\n",
    "# Input Web Graph:\n",
    "\n",
    "# A -> B, C\n",
    "# B -> C\n",
    "# C -> (no links)\n",
    "\n",
    "# Step 1: Fetch Links (from each URL):\n",
    "\n",
    "# A links to: B, C\n",
    "# B links to: C\n",
    "# C links to: (none)\n",
    "\n",
    "\n",
    "# Step 2: Build Web Graph:\n",
    "\n",
    "\n",
    "# web_graph = {\n",
    "#     'A': {'B', 'C'},\n",
    "#     'B': {'C'},\n",
    "#     'C': set()\n",
    "# }\n",
    "\n",
    "\n",
    "# Step 3: Create Adjacency Matrix:\n",
    "\n",
    "# The create_adjacency_matrix function:\n",
    "# Converts the web graph dictionary into a list of nodes (URLs).\n",
    "# Creates a square matrix (n x n).\n",
    "# For each URL:\n",
    "# If it links to another URL, it assigns 1 / number_of_links in the corresponding matrix position.\n",
    "# If a URL has no outgoing links, it assigns equal probability for each link to all other pages.\n",
    "\n",
    "# Nodes = ['A', 'B', 'C']\n",
    "# adjacency_matrix = [\n",
    "#     [0, 0, 1/3],\n",
    "#     [0.5, 0, 1/3],\n",
    "#     [0.5, 1, 1/3]\n",
    "# ]\n",
    "\n",
    "# Step 4: Calculate PageRank:\n",
    "\n",
    "# Initialize:\n",
    "\n",
    "# PR(A)=PR(B)=PR(C)= 1/3\n",
    "\n",
    "\n",
    "# Apply the formula iteratively:\n",
    "# After one iteration:\n",
    "# PR(A)=0.4333,PR(B)=0.4333,PR(C)=0.4333\n",
    "\n",
    "# Repeat until convergence. Final values after convergence:\n",
    "\n",
    "# PR(A)=0.4333,PR(B)=0.4333,PR(C)=0.4333\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
