{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a program for pre-processing of a text document such as stop word removal, stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Text: artifici intellig ai transform industri enabl new advanc healthcar financ transport ai help doctor make faster diagnos allow bank detect fraud even power car howev capabl rais ethic concern ai trust make critic decis prevent bia system machin learn branch ai allow comput learn data without explicit program technolog power tool like chatbot imag recognit softwar make digit interact natur despit benefit machin learn face challeng black box issu process becom hard explain ai autom advanc concern job displac aris role may autom ai also creat new career path data scienc technolog stay relev individu need focu skill ai easili replic creativ critic think\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import required libraries\n",
    "import nltk  # NLTK (Natural Language Toolkit) library for text processing\n",
    "from nltk.corpus import stopwords  # To get a list of stop words\n",
    "from nltk.tokenize import word_tokenize  # To split text into individual words\n",
    "from nltk.stem import PorterStemmer  # To reduce words to their root form\n",
    "\n",
    "# Step 2: Download necessary NLTK resources\n",
    "# These downloads are required to use certain NLTK functionalities like stopwords and tokenization\n",
    "nltk.download('punkt')  # 'punkt' is used for tokenizing sentences and words\n",
    "nltk.download('stopwords')  # English stop words list provided by NLTK\n",
    "\n",
    "# nltk.download('punkt'): Downloads a pre-trained tokenizer to split text into sentences or words, \n",
    "# enabling easier manipulation of text.\n",
    "\n",
    "# nltk.download('stopwords'): Downloads a list of common,\n",
    "#  low-value words to exclude from analysis to focus on more meaningful content.\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by removing stopwords and applying stemming.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The input text to preprocess.\n",
    "    \n",
    "    Returns:\n",
    "    - str: Processed text with stopwords removed and words stemmed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 3a: Tokenize text\n",
    "    # Tokenization splits text into individual words (tokens), which helps in further processing.\n",
    "    # We also convert text to lowercase to make processing case-insensitive.\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Step 3b: Initialize the PorterStemmer and load English stopwords\n",
    "    # PorterStemmer reduces words to their root form (e.g., \"running\" -> \"run\").\n",
    "    stemmer = PorterStemmer()\n",
    "    # stopwords.words('english') gives a set of common English stopwords like \"the\", \"is\", \"in\", etc.\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Step 3c: Remove stopwords and apply stemming\n",
    "    # We iterate through each token, remove punctuation, filter out stopwords, and apply stemming.\n",
    "    processed_words = [\n",
    "        stemmer.stem(word)  # Stems the word to its root form\n",
    "        for word in tokens  # For each word in the tokenized list\n",
    "        if word.isalnum() and word not in stop_words  # Only keep words that are alphanumeric and not stopwords\n",
    "    ]\n",
    "\n",
    "    # Step 3d: Join words back into a single string\n",
    "    # We join the processed list of words into a single string, separating each word with a space.\n",
    "    processed_text = ' '.join(processed_words)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# Step 4: Read text from a file and process it\n",
    "# Open the text file in read mode with UTF-8 encoding to avoid Unicode errors\n",
    "with open(\"Text.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    # Read the entire file content\n",
    "    text = file.read()\n",
    "\n",
    "# Process the loaded text by calling preprocess_text function\n",
    "processed_text = preprocess_text(text)\n",
    "\n",
    "# Display the processed text\n",
    "print(\"Processed Text:\", processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
